{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "initial_id",
            "metadata": {
                "collapsed": true,
                "jupyter": {
                    "outputs_hidden": true
                }
            },
            "source": [
                "## Lab 5: AgentCore Evaluations - Online Evaluation for Customer Support Agent\n",
                "\n",
                "### Overview\n",
                "\n",
                "This lab demonstrates how to use AgentCore Evaluations to continuously monitor your production customer support agent from Lab 4. You'll configure online evaluation to automatically assess agent performance in real-time as customers interact with it.\n",
                "\n",
                "**Workshop Journey:**\n",
                "\n",
                "- **Lab 1 (Done):** Create Agent Prototype - Built a functional customer support agent\n",
                "- **Lab 2 (Done):** Enhance with Memory - Added conversation context and personalization\n",
                "- **Lab 3 (Done):** Scale with Gateway & Identity - Shared tools across agents securely\n",
                "- **Lab 4 (Done):** Deploy to Production - Used AgentCore Runtime with observability\n",
                "- **Lab 5 (Current):** Evaluate Agent Performance - Monitor quality with online evaluations\n",
                "- **Lab 6:** Build User Interface - Create a customer-facing application\n",
                "\n",
                "### What You'll Learn\n",
                "\n",
                "You'll configure online evaluation with built-in evaluators, generate test interactions, and analyze quality metrics through AgentCore Observability dashboards to improve agent performance.\n",
                "\n",
                "### Online Evaluation Overview\n",
                "\n",
                "Online evaluation continuously monitors deployed agents in production, unlike on-demand evaluation which analyzes specific selected interactions. It consists of three components: session sampling with configurable rules, multiple evaluation methods (built-in or custom evaluators), and monitoring through dashboards with quality trends and low-scoring session investigation.\n",
                "\n",
                "Since your agent runs on AgentCore Runtime, AgentCore Observability automatically instruments the code and provides comprehensive logs and traces using [OTEL](https://opentelemetry.io/) instrumentation.\n",
                "\n",
                "### Prerequisites\n",
                "\n",
                "Complete Lab 4 to have the customer support agent deployed. You'll need AWS account access to Amazon Bedrock AgentCore with Evaluations permissions.\n",
                "\n",
                "### Architecture\n",
                "<div style=\"text-align:left\">\n",
                "    <img src=\"images/architecture_lab5_evaluation.png\" width=\"75%\"/>\n",
                "</div>\n",
                "\n",
                "*Online evaluation automatically monitors agent interactions, applies evaluators based on sampling rules, and outputs results to CloudWatch for analysis.*"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6d01af13-5b54-45eb-93f7-31d4d0974cce",
            "metadata": {},
            "source": [
                "### Step 1: Import Required Libraries and Initialize Clients"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3b7ff2f6-d0ee-4d11-8776-d9dbbc729dab",
            "metadata": {},
            "outputs": [],
            "source": [
                "from bedrock_agentcore_starter_toolkit import Evaluation, Runtime\n",
                "import json\n",
                "import uuid\n",
                "from pathlib import Path\n",
                "from boto3.session import Session\n",
                "from IPython.display import Markdown, display\n",
                "from lab_helpers.utils import get_ssm_parameter, get_or_create_cognito_pool"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "15ebd476-2e81-476c-b328-36a60d7b199d",
            "metadata": {},
            "outputs": [],
            "source": [
                "boto_session = Session()\n",
                "region = boto_session.region_name\n",
                "print(f\"Region: {region}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a06e1972-6cf3-4e07-adfd-cb4981902668",
            "metadata": {},
            "outputs": [],
            "source": [
                "eval_client = Evaluation(region=region)\n",
                "runtime_client = Runtime()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ddec431f-d5bd-47a1-b945-e29692dfaf14",
            "metadata": {},
            "source": [
                "### Step 2: Retrieve Agent Information from Lab 4\n",
                "\n",
                "Retrieve the customer support agent ARN from SSM Parameter Store where it was saved during Lab 4 deployment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c30d7001-3212-44b6-b401-ea93b8a9abc5",
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    # Get agent ARN from SSM parameter store (saved in Lab 4)\n",
                "    agent_arn = get_ssm_parameter(\"/app/customersupport/agentcore/runtime_arn\")\n",
                "    \n",
                "    # Extract agent ID from ARN\n",
                "    agent_id = agent_arn.split(\":\")[-1].split(\"/\")[-1]\n",
                "    \n",
                "    # Set runtime client config path\n",
                "    runtime_client._config_path = Path.cwd() / \".bedrock_agentcore.yaml\"\n",
                "    \n",
                "    print(\"Agent ID:\", agent_id)\n",
                "    print(\"Agent ARN:\", agent_arn)\n",
                "except Exception as e:\n",
                "    raise Exception(f\"\"\"Missing agent information from Lab 4. Please run lab-04-agentcore-runtime.ipynb first. Error: {str(e)}\"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c07d6132-71d7-4654-9310-a61bdc435bc9",
            "metadata": {},
            "source": [
                "### Step 3: Create Online Evaluation Configuration\n",
                "\n",
                "Now let's create an online evaluation configuration for our customer support agent. We'll use built-in evaluators to assess different aspects of agent performance:\n",
                "\n",
                "- **Builtin.GoalSuccessRate** - Measures how well the agent achieves user goals\n",
                "- **Builtin.Correctness** - Evaluates factual accuracy of responses\n",
                "- **Builtin.ToolSelectionAccuracy** - Evaluates appropriate tool selection\n",
                "\n",
                "We'll set the sampling rate to 100% for demonstration purposes, but in production you might use a lower rate (e.g., 10-20%) based on your traffic volume."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1921ef17-43e4-4471-92b9-a0f31bd8f018",
            "metadata": {},
            "outputs": [],
            "source": [
                "response = eval_client.create_online_config(\n",
                "    agent_id=agent_id,\n",
                "    config_name=\"customer_support_agent_eval\",\n",
                "    sampling_rate=100,  # Evaluate 100% of sessions for demo\n",
                "    evaluator_list=[\n",
                "        \"Builtin.GoalSuccessRate\", \n",
                "        \"Builtin.Correctness\",\n",
                "        \"Builtin.ToolSelectionAccuracy\"\n",
                "    ],\n",
                "    config_description=\"Customer support agent online evaluation\",\n",
                "    auto_create_execution_role=True\n",
                ")\n",
                "\n",
                "print(\"Online evaluation configuration created successfully!\")\n",
                "print(f\"Configuration ID: {response['onlineEvaluationConfigId']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "746ac193-5672-473d-a4c2-668667c1e77e",
            "metadata": {},
            "source": [
                "### Step 4: Verify Configuration Status\n",
                "\n",
                "Verify the evaluation configuration is properly created and enabled by retrieving its details."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "56b8ee2f-f875-41a8-8226-650d1127e5ce",
            "metadata": {},
            "outputs": [],
            "source": [
                "config_details = eval_client.get_online_config(config_id=response['onlineEvaluationConfigId'])\n",
                "print(\"Configuration Details:\")\n",
                "print(json.dumps(config_details, indent=2, default=str))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "257f892c-b15a-497e-a454-cb1677468725",
            "metadata": {},
            "source": [
                "### Step 5: Generate Test Interactions\n",
                "\n",
                "Invoke the customer support agent with various queries to generate traces for evaluation. Different test scenarios will demonstrate how the evaluators assess agent performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4f88a905-767f-49e9-9978-66ae5845343f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get authentication token\n",
                "access_token = get_or_create_cognito_pool(refresh_token=True)\n",
                "print(f\"Access token obtained: {access_token['bearer_token'][:20]}...\")\n",
                "\n",
                "def invoke_agent_runtime(prompt, session_id=None):\n",
                "    \"\"\"Invoke the agent runtime using starter toolkit\"\"\"\n",
                "    if not session_id:\n",
                "        session_id = str(uuid.uuid4())\n",
                "    \n",
                "    response = runtime_client.invoke(\n",
                "        payload={\"prompt\": prompt},\n",
                "        session_id=session_id,\n",
                "        bearer_token=access_token['bearer_token']\n",
                "    )\n",
                "    \n",
                "    return response, session_id"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "531b72f0-8c4b-4b27-b0ca-38c22d666cbd",
            "metadata": {},
            "source": [
                "#### Test Scenario 1: Product Information Query"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6dbda8d0-dc8d-4793-90ed-63760b0ab094",
            "metadata": {},
            "outputs": [],
            "source": [
                "session1 = str(uuid.uuid4())\n",
                "response, _ = invoke_agent_runtime(\n",
                "    \"I need information about the Gaming Console Pro. What are its specifications and price?\",\n",
                "    session1\n",
                ")\n",
                "print(\"Customer Query: Product information request\")\n",
                "display(Markdown(response[\"response\"].replace('\\\\n', '\\n')))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1de72c8a-0ac8-40af-9604-b5a730ef803d",
            "metadata": {},
            "source": [
                "#### Test Scenario 2: Technical Support Request"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2f903cc1-e025-4a67-bfeb-eed6b8f6f326",
            "metadata": {},
            "outputs": [],
            "source": [
                "session2 = str(uuid.uuid4())\n",
                "response, _ = invoke_agent_runtime(\n",
                "    \"My laptop won't start up. Can you help me troubleshoot this issue?\",\n",
                "    session2\n",
                ")\n",
                "print(\"Customer Query: Technical support request\")\n",
                "display(Markdown(response[\"response\"].replace('\\\\n', '\\n')))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "94d85101-dd53-40a0-8d42-403742c7eed3",
            "metadata": {},
            "source": [
                "#### Test Scenario 3: Return Policy Inquiry"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f7fc5937-b248-47bc-a596-48f46728bb13",
            "metadata": {},
            "outputs": [],
            "source": [
                "session3 = str(uuid.uuid4())\n",
                "response, _ = invoke_agent_runtime(\n",
                "    \"I bought a smartphone last week but it's not working properly. What's your return policy?\",\n",
                "    session3\n",
                ")\n",
                "print(\"Customer Query: Return policy inquiry\")\n",
                "display(Markdown(response[\"response\"].replace('\\\\n', '\\n')))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8d4bbc31-8f9c-4685-9db4-e59a500c78f3",
            "metadata": {},
            "source": [
                "#### Test Scenario 4: Complex Multi-Tool Query"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a1b2c3d4-e5f6-7890-1234-567890abcdef",
            "metadata": {},
            "outputs": [],
            "source": [
                "session4 = str(uuid.uuid4())\n",
                "response, _ = invoke_agent_runtime(\n",
                "    \"I need help with my Gaming Console Pro. First, can you tell me about its warranty? Then I need technical support for connection issues.\",\n",
                "    session4\n",
                ")\n",
                "print(\"Customer Query: Complex multi-tool request\")\n",
                "display(Markdown(response[\"response\"].replace('\\\\n', '\\n')))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b1c2d3e4-f5g6-h789-i012-j345k678l901",
            "metadata": {},
            "source": [
                "#### Test Scenario 5: General Capability Query"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "m2n3o4p5-q6r7-s890-t123-u456v789w012",
            "metadata": {},
            "outputs": [],
            "source": [
                "session5 = str(uuid.uuid4())\n",
                "response, _ = invoke_agent_runtime(\n",
                "    \"What kind of support can you provide? List all your available tools and capabilities.\",\n",
                "    session5\n",
                ")\n",
                "print(\"Customer Query: Capability inquiry\")\n",
                "display(Markdown(response[\"response\"].replace('\\\\n', '\\n')))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "x3y4z5a6-b7c8-d901-e234-f567g890h123",
            "metadata": {},
            "source": [
                "### Step 6: Monitor Evaluation Results\n",
                "\n",
                "Monitor evaluation results through the AgentCore Observability console. Results may take a few minutes to appear as the system processes traces and applies evaluators.\n",
                "\n",
                "#### Accessing the Dashboard\n",
                "\n",
                "1. Navigate to the [AgentCore Observability console](https://console.aws.amazon.com/cloudwatch/home#gen-ai-observability/agent-core/agents)\n",
                "2. Find your customer support agent in the agents list\n",
                "3. Click on the `DEFAULT` endpoint to view evaluation metrics\n",
                "4. Look for the evaluation scores in the traces and sessions views\n",
                "\n",
                "#### What You'll See\n",
                "\n",
                "The dashboard will show:\n",
                "- **Goal Success Rate**: How well the agent achieves customer objectives\n",
                "- **Correctness**: Accuracy of information provided\n",
                "- **Tool Selection Accuracy**: Appropriate tool choices for queries\n",
                "\n",
                "![Online Evaluation Dashboard](images/online_evaluations_dashboard.png)\n",
                "\n",
                "*Evaluation metrics displayed in the AgentCore Observability dashboard*"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "i4j5k6l7-m8n9-o012-p345-q678r901s234",
            "metadata": {},
            "source": [
                "### Step 7: Understanding Evaluation Metrics\n",
                "\n",
                "**Goal Success Rate** measures whether the agent successfully addresses the customer's primary intent. High scores indicate effective problem-solving; low scores suggest unmet needs, incomplete responses, or misunderstood requests.\n",
                "\n",
                "**Correctness** evaluates factual accuracy of responses. High scores indicate accurate and reliable information; low scores suggest incorrect facts, outdated information, or misleading guidance.\n",
                "\n",
                "**Tool Selection Accuracy** evaluates whether the agent chooses appropriate tools for each task. High scores indicate proper tool selection; low scores suggest wrong tools, unnecessary calls, or missing tool usage."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "v5w6x7y8-z9a0-b123-c456-d789e012f345",
            "metadata": {},
            "source": [
                "### Step 8: Analyzing Results and Next Steps\n",
                "\n",
                "**For Low Goal Success Rates:** Refine the agent's system prompt, improve tool descriptions and parameters, and add specific training examples.\n",
                "\n",
                "**For Low Correctness Scores:** Update the knowledge base with current information, improve fact-checking mechanisms, and review tool responses.\n",
                "\n",
                "**For Tool-Related Issues:** Refine tool parameter schemas, improve tool selection logic, and enhance tool documentation.\n",
                "\n",
                "**Continuous Monitoring:** Set up CloudWatch alarms for evaluation metrics, create dashboards for trend analysis, and implement automated alerts for quality degradation."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "g6h7i8j9-k0l1-m234-n567-o890p123q456",
            "metadata": {},
            "source": [
                "### Step 9: Clean Up (Optional)\n",
                "\n",
                "Disable the online evaluation configuration if needed by uncommenting the code below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "r7s8t9u0-v1w2-x345-y678-z901a234b567",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment the following lines if you want to disable the evaluation configuration\n",
                "# eval_client.delete_online_config(config_id=response['onlineEvaluationConfigId'])\n",
                "# print(\"Online evaluation configuration disabled\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c8d9e0f1-g2h3-i456-j789-k012l345m678",
            "metadata": {},
            "source": [
                "### Congratulations! ðŸŽ‰\n",
                "\n",
                "You have successfully completed **Lab 5: AgentCore Evaluations - Online Evaluation!**\n",
                "\n",
                "### What You Accomplished\n",
                "\n",
                "You configured automatic continuous online evaluation for your customer support agent with built-in evaluators assessing Goal Success Rate (customer satisfaction and problem resolution), Correctness (factual accuracy), and Tool Selection Accuracy (proper tool usage). Evaluation results are integrated with AgentCore Observability dashboards for real-time insights.\n",
                "\n",
                "**Key Benefits:** Proactive quality assurance catches issues before customer impact, data-driven optimization guides improvements, production confidence through performance monitoring at scale, and continuous learning identifies patterns and opportunities.\n",
                "\n",
                "**Next Steps:** Monitor your evaluation dashboard regularly, set up CloudWatch alarms for quality thresholds, use insights to iteratively improve your agent, and consider adding custom evaluators for domain-specific metrics.\n",
                "\n",
                "### Next Up: [Lab 6: Build User Interface â†’](lab-06-frontend.ipynb)\n",
                "\n",
                "Complete the customer experience by building a user-friendly web interface for customers to interact with your quality-monitored agent.\n",
                "\n",
                "Your customer support agent is now production-ready with comprehensive quality monitoring! ðŸš€"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
