{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c0122e65c053f38",
   "metadata": {},
   "source": [
    "# Hosting LlamaIndex Agents with Amazon Bedrock models in Amazon Bedrock AgentCore Runtime with Observability\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this tutorial we will learn how to host your LlamaIndex agent using Amazon Bedrock AgentCore Runtime with built-in observability. This tutorial demonstrates how to deploy a LlamaIndex agent to AgentCore Runtime and automatically capture telemetry data for monitoring and analysis.\n",
    "\n",
    "### Tutorial Details\n",
    "\n",
    "| Information         | Details                                                                          |\n",
    "|:--------------------|:---------------------------------------------------------------------------------|\n",
    "| Tutorial type       | Conversational                                                                   |\n",
    "| Agent type          | Single                                                                           |\n",
    "| Agentic Framework   | LlamaIndex                                                                       |\n",
    "| LLM model           | Anthropic Claude Haiku                                                           |\n",
    "| Tutorial components | Hosting agent on AgentCore Runtime with Observability                           |\n",
    "| Tutorial vertical   | Cross-vertical                                                                   |\n",
    "| Example complexity  | Easy                                                                             |\n",
    "| SDK used            | Amazon BedrockAgentCore Python SDK and boto3                                    |\n",
    "\n",
    "### Tutorial Architecture\n",
    "\n",
    "In this tutorial we will describe how to deploy a LlamaIndex agent to AgentCore runtime with automatic observability.\n",
    "\n",
    "For demonstration purposes, we will use a LlamaIndex FunctionAgent using Amazon Bedrock models with arithmetic tools.\n",
    "\n",
    "### Tutorial Key Features\n",
    "\n",
    "* Hosting LlamaIndex Agents on Amazon Bedrock AgentCore Runtime\n",
    "* Using Amazon Bedrock models\n",
    "* Automatic observability and tracing\n",
    "* Built-in telemetry collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a676f58ecf52b42",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To execute this tutorial you will need:\n",
    "* Python 3.10+\n",
    "* AWS credentials\n",
    "* Amazon Bedrock AgentCore SDK\n",
    "* LlamaIndex\n",
    "* Amazon Bedrock Model access to Claude Haiku"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758cc77a",
   "metadata": {},
   "source": [
    "### In Your Terminal:\n",
    "\n",
    "`cd 01-tutorials/06-AgentCore-observability/01-Agentcore-runtime-hosted/LlamaIndex`\n",
    "\n",
    "`python -m venv venv`\n",
    "\n",
    "`source venv/bin/activate`\n",
    "\n",
    "### In Your Notebook:\n",
    "\n",
    "Select your venv as your kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q --force-reinstall -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca924a7a2731e26f",
   "metadata": {},
   "source": [
    "## Creating your LlamaIndex agent and experimenting locally\n",
    "\n",
    "Before we deploy our agent to AgentCore Runtime, let's develop and run it locally for experimentation purposes.\n",
    "\n",
    "For production agentic applications we will need to decouple the agent creation process from the agent invocation one. With AgentCore Runtime, we will decorate the invocation part of our agent with the `@app.entrypoint` decorator and have it as the entry point for our runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d386ab54e85e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llamaindex_agent.py\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*validate_default.*\", category=UserWarning)\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import boto3\n",
    "from llama_index.llms.bedrock_converse import BedrockConverse\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.observability.otel import LlamaIndexOpenTelemetry\n",
    "\n",
    "\n",
    "# Initialize OpenTelemetry instrumentation for LlamaIndex\n",
    "instrumentor = LlamaIndexOpenTelemetry()\n",
    "# Start listening\n",
    "instrumentor.start_registering()\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def get_bedrock_model():\n",
    "    model_id = \"anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "    region = boto3.Session().region_name\n",
    "    \n",
    "    bedrock_model = BedrockConverse(\n",
    "        model=model_id,\n",
    "        region_name=region,\n",
    "    )\n",
    "    return bedrock_model\n",
    "\n",
    "# Initialize the model\n",
    "bedrock_model = get_bedrock_model()\n",
    "\n",
    "# Create the arithmetic agent\n",
    "agent = FunctionAgent(\n",
    "    tools=[add, multiply],\n",
    "    llm=bedrock_model,\n",
    ")\n",
    "\n",
    "async def llamaindex_agent_bedrock(payload):\n",
    "    \"\"\"\n",
    "    Invoke the agent with a payload\n",
    "    \"\"\"\n",
    "    user_input = payload.get(\"prompt\")\n",
    "    response = await agent.run(user_input)\n",
    "    return str(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"payload\", type=str)\n",
    "    args = parser.parse_args()\n",
    "    response = asyncio.run(llamaindex_agent_bedrock(json.loads(args.payload)))\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68499675-db8d-47c6-8c0c-5d66dcb06229",
   "metadata": {},
   "source": [
    "#### Invoking local agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1226d59e6b56c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python llamaindex_agent.py '{\"prompt\": \"What is (121 + 2) * 5?\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932110e6-fca6-47b6-b7c5-c4714a866a80",
   "metadata": {},
   "source": [
    "## Preparing your agent for deployment on AgentCore Runtime\n",
    "\n",
    "Let's now deploy our agent to AgentCore Runtime. To do so we need to:\n",
    "* Import the Runtime App with `from bedrock_agentcore.runtime import BedrockAgentCoreApp`\n",
    "* Initialize the App in our code with `app = BedrockAgentCoreApp()`\n",
    "* Decorate the invocation function with the `@app.entrypoint` decorator\n",
    "* Let AgentCoreRuntime control the running of the agent with `app.run()`\n",
    "\n",
    "### LlamaIndex Agent with Amazon Bedrock model\n",
    "Let's prepare our LlamaIndex Agent for AgentCore Runtime deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b845b32-a03e-45c2-a2f0-2afba8069f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llamaindex_agent.py\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*validate_default.*\", category=UserWarning)\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from bedrock_agentcore.runtime import BedrockAgentCoreApp\n",
    "from llama_index.llms.bedrock_converse import BedrockConverse\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.observability.otel import LlamaIndexOpenTelemetry\n",
    "\n",
    "\n",
    "app = BedrockAgentCoreApp()\n",
    "\n",
    "# Initialize OpenTelemetry instrumentation for LlamaIndex\n",
    "instrumentor = LlamaIndexOpenTelemetry(debug=True)\n",
    "# Start listening\n",
    "instrumentor.start_registering()\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def get_bedrock_model():\n",
    "    model_id = \"anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "    region = boto3.Session().region_name\n",
    "    \n",
    "    bedrock_model = BedrockConverse(\n",
    "        model=model_id,\n",
    "        region_name=region,\n",
    "    )\n",
    "    return bedrock_model\n",
    "\n",
    "# Initialize the model\n",
    "bedrock_model = get_bedrock_model()\n",
    "\n",
    "# Create the arithmetic agent\n",
    "agent = FunctionAgent(\n",
    "    tools=[add, multiply],\n",
    "    llm=bedrock_model,\n",
    ")\n",
    "\n",
    "@app.entrypoint\n",
    "async def llamaindex_agent_bedrock(payload):\n",
    "    \"\"\"\n",
    "    Invoke the agent with a payload\n",
    "    \"\"\"\n",
    "    user_input = payload.get(\"prompt\")\n",
    "    print(\"User input:\", user_input)\n",
    "    response = await agent.run(user_input)\n",
    "    return str(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64db7b5-0f1b-475f-9bf2-467b4449d46a",
   "metadata": {},
   "source": [
    "## What happens behind the scenes?\n",
    "\n",
    "When you use `BedrockAgentCoreApp`, it automatically:\n",
    "\n",
    "* Creates an HTTP server that listens on the port 8080\n",
    "* Implements the required `/invocations` endpoint for processing the agent's requirements\n",
    "* Implements the `/ping` endpoint for health checks\n",
    "* Handles proper content types and response formats\n",
    "* Manages error handling according to the AWS standards\n",
    "* **Automatically enables observability and telemetry collection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6820ca8f-a8a8-4f34-b4ef-b6dad3776261",
   "metadata": {},
   "source": [
    "## Deploying the agent to AgentCore Runtime\n",
    "\n",
    "The `CreateAgentRuntime` operation supports comprehensive configuration options, letting you specify container images, environment variables and encryption settings. You can also configure protocol settings (HTTP, MCP) and authorization mechanisms to control how your clients communicate with the agent.\n",
    "\n",
    "**Note:** Operations best practice is to package code as container and push to ECR using CI/CD pipelines and IaC\n",
    "\n",
    "In this tutorial we will use the Amazon Bedrock AgentCore Python SDK to easily package your artifacts and deploy them to AgentCore runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8855aceb-b79f-4aaa-b16f-8577c059816a",
   "metadata": {},
   "source": [
    "### Configure AgentCore Runtime deployment\n",
    "\n",
    "First we will use our starter toolkit to configure the AgentCore Runtime deployment with an entrypoint, the execution role we just created and a requirements file. We will also configure the starter kit to auto create the Amazon ECR repository on launch.\n",
    "\n",
    "During the configure step, your docker file will be generated based on your application code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e79eba2-ca59-463f-9ebf-56e362d7ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bedrock_agentcore_starter_toolkit.notebook.runtime.bedrock_agentcore import Runtime\n",
    "from boto3.session import Session\n",
    "boto_session = Session()\n",
    "region = boto_session.region_name\n",
    "\n",
    "agentcore_runtime = Runtime()\n",
    "agent_name = \"llamaindex_bedrock_getting_started10\"\n",
    "response = agentcore_runtime.configure(\n",
    "    entrypoint=\"llamaindex_agent.py\",\n",
    "    auto_create_execution_role=True,\n",
    "    auto_create_ecr=True,\n",
    "    requirements_file=\"requirements.txt\",\n",
    "    region=region,\n",
    "    agent_name=agent_name\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1b84cc-798e-472c-ac0b-2c315f4b704d",
   "metadata": {},
   "source": [
    "### Launching agent to AgentCore Runtime\n",
    "\n",
    "Now that we've got a docker file, let's launch the agent to the AgentCore Runtime. This will create the Amazon ECR repository and the AgentCore Runtime with observability automatically enabled. You can add libraries to the environment variable below to exclude unnecessary traces from specific libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a32ab8-7701-4900-8055-e24364bdf35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_result = agentcore_runtime.launch(\n",
    "    env_vars={\n",
    "        # Minimal set - only disable truly noisy instrumentations\n",
    "        \"OTEL_PYTHON_DISABLED_INSTRUMENTATIONS\": (\n",
    "            \"jinja2,\"\n",
    "            \"urllib3,\"\n",
    "            \"requests,\"\n",
    "            \"httpx,\"\n",
    "            \"redis,\"\n",
    "            \"aiohttp-client\"\n",
    "            # Add \"starlette\" to this list to get rid of the POST /invocations trace. Note: this will disable session tracking.\n",
    "        )\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae9c09-09db-4a76-871a-92eacd96b9c3",
   "metadata": {},
   "source": [
    "### Checking for the AgentCore Runtime Status\n",
    "Now that we've deployed the AgentCore Runtime, let's check for its deployment status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa6ac09-9adb-4846-9fc1-4d12aeb74853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "status_response = agentcore_runtime.status()\n",
    "status = status_response.endpoint['status']\n",
    "end_status = ['READY', 'CREATE_FAILED', 'DELETE_FAILED', 'UPDATE_FAILED']\n",
    "while status not in end_status:\n",
    "    time.sleep(10)\n",
    "    status_response = agentcore_runtime.status()\n",
    "    status = status_response.endpoint['status']\n",
    "    print(status)\n",
    "status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eb5ca3",
   "metadata": {},
   "source": [
    "## Enable Tracing for your AgentCore Runtime\n",
    "\n",
    "In your AWS console, navigate to Amazon Bedrock AgentCore. Click on Agent Runtime under Build and Deploy and select your agent.\n",
    "\n",
    "For your agent, scroll until you see the Tracing section. Enable this to allow for trace delivery to CloudWatch:\n",
    "\n",
    "![enable_tracing.png](images/llamaindex_enable_tracing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f89c56-918a-4cab-beaa-c7ac43a2ba29",
   "metadata": {},
   "source": [
    "### Invoking AgentCore Runtime\n",
    "\n",
    "Finally, we can invoke our AgentCore Runtime with a payload. This will automatically generate telemetry data that can be viewed in the observability dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d909e42-e1a0-407f-84c2-3d16cc889cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_response = agentcore_runtime.invoke({\"prompt\": \"What is (121 + 2) * 5?\"})\n",
    "invoke_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefa09f2-d25a-483f-aedb-11690bb8923a",
   "metadata": {},
   "source": [
    "### Processing invocation results\n",
    "\n",
    "We can now process our invocation results to include it in an application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11249103-cfb3-47b5-970d-981a977a225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import json\n",
    "response_text = invoke_response['response'][0]\n",
    "display(Markdown(response_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1d2bce-be41-478c-8bed-b4037c385795",
   "metadata": {},
   "source": [
    "### Invoking AgentCore Runtime with boto3\n",
    "\n",
    "Now that your AgentCore Runtime was created you can invoke it with any AWS SDK. For instance, you can use the boto3 `invoke_agent_runtime` method for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f84e68d-6c04-41b9-bf5b-60edc3fa0985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "agent_arn = launch_result.agent_arn\n",
    "agentcore_client = boto3.client(\n",
    "    'bedrock-agentcore',\n",
    "    region_name=region\n",
    ")\n",
    "\n",
    "boto3_response = agentcore_client.invoke_agent_runtime(\n",
    "    agentRuntimeArn=agent_arn,\n",
    "    qualifier=\"DEFAULT\",\n",
    "    payload=json.dumps({\"prompt\": \"What is 15 * 8?\"})\n",
    ")\n",
    "if \"text/event-stream\" in boto3_response.get(\"contentType\", \"\"):\n",
    "    content = []\n",
    "    for line in boto3_response[\"response\"].iter_lines(chunk_size=1):\n",
    "        if line:\n",
    "            line = line.decode(\"utf-8\")\n",
    "            if line.startswith(\"data: \"):\n",
    "                line = line[6:]\n",
    "                print(line)\n",
    "                content.append(line)\n",
    "    display(Markdown(\"\\n\".join(content)))\n",
    "else:\n",
    "    try:\n",
    "        events = []\n",
    "        for event in boto3_response.get(\"response\", []):\n",
    "            events.append(event)\n",
    "    except Exception as e:\n",
    "        events = [f\"Error reading EventStream: {e}\"]\n",
    "    display(Markdown(json.loads(events[0].decode(\"utf-8\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "observability_section",
   "metadata": {},
   "source": [
    "## Observability Dashboard\n",
    "\n",
    "### Automatic Telemetry Collection\n",
    "\n",
    "When your LlamaIndex agent runs on AgentCore Runtime, telemetry data is automatically collected and sent to Amazon CloudWatch. This includes:\n",
    "\n",
    "- **Agent execution traces**: Complete workflow of your agent's decision-making process\n",
    "- **LLM calls**: Bedrock model invocations with input/output tokens\n",
    "- **Tool usage**: Function calls and their results\n",
    "- **Performance metrics**: Latency, token usage, and error rates\n",
    "\n",
    "### Viewing Traces in CloudWatch\n",
    "\n",
    "To view your agent's observability data:\n",
    "\n",
    "1. Navigate to the AWS CloudWatch console\n",
    "2. Go to **GenAI Observability** dashboard\n",
    "3. Select your agent runtime to view traces and metrics\n",
    "\n",
    "### Key Observability Features\n",
    "\n",
    "- **Session tracking**: Correlate multiple interactions\n",
    "- **Error monitoring**: Identify and debug issues\n",
    "- **Performance analysis**: Optimize agent response times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd1713",
   "metadata": {},
   "source": [
    "### AgentCore Observability on Amazon CloudWatch\n",
    "\n",
    "To summarize, please follow the below steps to enable observability from AgentCore runtime hosted agents:\n",
    "\n",
    "- Enable Transaction Search on Amazon CloudWatch\n",
    "- The requirements.txt file contains `aws-opentelemetry-distro` listed while deploying the agent on Bedrock AgentCore Runtime.\n",
    "\n",
    "## Bedrock AgentCore Overview on GenAI Observability dashboard\n",
    "\n",
    "You are able to view all your Agents that have observability in them and filter the data based on time frames, some examples are provided below:\n",
    "\n",
    "![genai-observability.png](images/llamaindex_dashboard_view.png)\n",
    "\n",
    "In the main dashboard you are able to view runtime metrics across all agents as shown below:\n",
    "\n",
    "![runtime-all-agent-metrics.png](images/llamaindex_runtime_metrics_total_view.png)\n",
    "\n",
    "Now, if you click on the agent you just deployed you will be taken to a dashboard for the runtime metrics specific to this agent, you can also filter the data by a custom time frame:\n",
    "\n",
    "![runtime-metrics-per-agent.png](images/llamaindex_runtime_metrics_view.png)\n",
    "\n",
    "In the Sessions View tab, you can navigate to all the sessions associated with this agent:\n",
    "\n",
    "![Agent-sessions-view.png](images/llamaindex_sessions_view.png)\n",
    "\n",
    "In the Trace View tab, you can look into the traces and span information for this agent on runtime:\n",
    "\n",
    "![Agentcore-trace.png](images/llamaindex_traces_view.png)\n",
    "\n",
    "Please click through the various features of GenAI observability dashboard to get more detailed information on traces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3fdfe404469632",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "Let's now clean up the AgentCore Runtime created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c243e86-a214-483c-aef1-d5243f28ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_result.ecr_uri, launch_result.agent_id, launch_result.ecr_uri.split('/')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a6cf1416830a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "agentcore_control_client = boto3.client(\n",
    "    'bedrock-agentcore-control',\n",
    "    region_name=region\n",
    ")\n",
    "ecr_client = boto3.client(\n",
    "    'ecr',\n",
    "    region_name=region\n",
    "    \n",
    ")\n",
    "\n",
    "runtime_delete_response = agentcore_control_client.delete_agent_runtime(\n",
    "    agentRuntimeId=launch_result.agent_id,\n",
    "    \n",
    ")\n",
    "\n",
    "response = ecr_client.delete_repository(\n",
    "    repositoryName=launch_result.ecr_uri.split('/')[1],\n",
    "    force=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118ad38-feeb-4d1d-9d57-e5c845becc56",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "You have successfully:\n",
    "\n",
    "- Created a LlamaIndex agent with arithmetic tools\n",
    "- Deployed it to Amazon Bedrock AgentCore Runtime\n",
    "- Enabled automatic observability and telemetry collection\n",
    "- Invoked the agent and generated trace data\n",
    "\n",
    "Your agent is now running with full observability capabilities, allowing you to monitor performance, debug issues, and optimize your agentic applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
